{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing directory to access the database\n",
    "import os\n",
    "os.chdir(\"/Users/komangandikawirasantosa/Face-Rec/DeepFace-IPYNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CoreMLExecutionProvider, AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface import app\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the InsightFace model\n",
    "face_recognition = app.FaceAnalysis()\n",
    "face_recognition.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# Load the face database\n",
    "database_path = 'cropped_faces'  # Replace with the path to your database\n",
    "face_embeddings = {}\n",
    "for person_name in os.listdir(database_path):\n",
    "    person_folder = os.path.join(database_path, person_name)\n",
    "    if os.path.isdir(person_folder):\n",
    "        face_embeddings[person_name] = []\n",
    "        for img_name in os.listdir(person_folder):\n",
    "            img_path = os.path.join(person_folder, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                faces = face_recognition.get(img)\n",
    "                for face in faces:\n",
    "                    # Store the normalized embedding of the first face detected\n",
    "                    face_embeddings[person_name].append(face.normed_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    }
   ],
   "source": [
    "# Load the target image for comparison\n",
    "target_image_path = 'Test1.jpeg'  # Replace with the path to your target image\n",
    "target_image = cv2.imread(target_image_path)\n",
    "\n",
    "if target_image is None:\n",
    "    print(\"Error: Could not read target image.\")\n",
    "    exit()\n",
    "\n",
    "# Perform face detection and recognition on the target image\n",
    "faces = face_recognition.get(target_image)\n",
    "\n",
    "# Display results\n",
    "for face in faces:\n",
    "    # Get bounding box and landmarks\n",
    "    bbox = face.bbox.astype(int)\n",
    "    identity = face.normed_embedding\n",
    "\n",
    "    # Compare with the database\n",
    "    recognized_person = None\n",
    "    min_distance = float('inf')\n",
    "    for person_name, embeddings in face_embeddings.items():\n",
    "        for embedding in embeddings:\n",
    "            distance = cosine(identity, embedding)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                recognized_person = person_name\n",
    "\n",
    "    # Draw bounding box and label on the image\n",
    "    if recognized_person is not None and min_distance < 0.5:  # Adjust threshold as necessary\n",
    "        label = recognized_person\n",
    "    else:\n",
    "        label = \"Unknown\"\n",
    "\n",
    "    cv2.rectangle(target_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    cv2.putText(target_image, label, (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Show the resulting image\n",
    "cv2.imshow('Face Recognition', target_image)\n",
    "cv2.waitKey(0)  # Wait until a key is pressed\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing directory to access the database\n",
    "import os\n",
    "os.chdir(\"/Users/komangandikawirasantosa/Face-Rec/DeepFace-IPYNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/komangandikawirasantosa/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "import insightface\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the InsightFace model\n",
    "model = insightface.app.FaceAnalysis(providers=['CPUExecutionProvider'])\n",
    "model.prepare(ctx_id=0)\n",
    "\n",
    "# Directory to store embeddings\n",
    "embedding_dir = \"face_embeddings_v2\"\n",
    "\n",
    "def create_and_store_embeddings(image_dir):\n",
    "    \"\"\"\n",
    "    Generate and store embeddings for each person.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(embedding_dir):\n",
    "        os.makedirs(embedding_dir)\n",
    "\n",
    "    for person_name in os.listdir(image_dir):\n",
    "        person_dir = os.path.join(image_dir, person_name)\n",
    "\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # Process each image of the person\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Unable to read image {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Resize the image to a larger size (e.g., 112x112 or 224x224)\n",
    "            img = cv2.resize(img, (112, 112))  # Adjust size as needed\n",
    "\n",
    "            # Get the embedding directly from the image\n",
    "            try:\n",
    "                face = model.get(img)  # This should return embeddings directly\n",
    "                if len(face) > 0:\n",
    "                    embeddings.append(face[0].normed_embedding)  # Get the embedding of the first face\n",
    "                else:\n",
    "                    print(f\"No faces found in {image_path}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "        if embeddings:\n",
    "            with open(os.path.join(embedding_dir, f\"{person_name}.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(embeddings, f)\n",
    "            print(f\"Saved embeddings for {person_name}\")\n",
    "        else:\n",
    "            print(f\"No embeddings found for {person_name}. Skipping.\")\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"\n",
    "    Load stored embeddings of each person for recognition.\n",
    "    \"\"\"\n",
    "    known_embeddings = {}\n",
    "    for file_name in os.listdir(embedding_dir):\n",
    "        person_name = file_name.split(\".pkl\")[0]\n",
    "        with open(os.path.join(embedding_dir, file_name), \"rb\") as f:\n",
    "            known_embeddings[person_name] = pickle.load(f)\n",
    "    return known_embeddings\n",
    "\n",
    "def recognize_faces(test_image_path, output_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Recognize faces in the test image, annotate them with name and distance, and save the result.\n",
    "    \"\"\"\n",
    "    known_embeddings = load_embeddings()\n",
    "\n",
    "    # Load the test image\n",
    "    img = cv2.imread(test_image_path)\n",
    "    faces = model.get(img)\n",
    "\n",
    "    # Iterate over each detected face\n",
    "    for face in faces:\n",
    "        # Use normed_embedding\n",
    "        embedding = face.normed_embedding\n",
    "        min_distance = float(\"inf\")\n",
    "        identity = \"Unknown\"\n",
    "\n",
    "        # Compare embedding with known embeddings\n",
    "        for person_name, person_embeddings in known_embeddings.items():\n",
    "            for stored_embedding in person_embeddings:\n",
    "                # Compute cosine distance\n",
    "                distance = cosine(embedding, stored_embedding)\n",
    "\n",
    "                if distance < min_distance and distance < threshold:\n",
    "                    min_distance = distance\n",
    "                    identity = person_name\n",
    "\n",
    "        # Draw bounding box, name, and distance\n",
    "        box = face.bbox.astype(int)\n",
    "        cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display identity and distance\n",
    "        label = f\"{identity} ({min_distance:.2f})\"\n",
    "        cv2.putText(img, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # Save the result image\n",
    "    cv2.imwrite(output_path, img)\n",
    "    print(f\"Result saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "# create_and_store_embeddings(\"path_to_your_cropped_images\")\n",
    "# recognize_faces(\"path_to_test_image.jpg\", \"output_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/komangandikawirasantosa/opt/anaconda3/lib/python3.9/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces found in Database/Tania/Tania.jpg.\n",
      "Saved embeddings for Tania\n",
      "No faces found in Database/Safira-Sama/Saf1.png.\n",
      "Saved embeddings for Safira-Sama\n",
      "Saved embeddings for Deci\n",
      "Saved embeddings for Elen\n",
      "No faces found in Database/Rachma/Rach3.png.\n",
      "No faces found in Database/Rachma/Rach2.png.\n",
      "No faces found in Database/Rachma/Rachma.jpg.\n",
      "Saved embeddings for Rachma\n",
      "Warning: Unable to read image Database/Andika/.DS_Store. Skipping.\n",
      "No faces found in Database/Andika/Andi2.JPG.\n",
      "Saved embeddings for Andika\n",
      "No faces found in Database/Patricia/Patricia.jpg.\n",
      "Saved embeddings for Patricia\n",
      "Saved embeddings for Jumaga\n",
      "Saved embeddings for BangRizal\n",
      "No faces found in Database/Maul-Unyu/Maulana.jpg.\n",
      "No faces found in Database/Maul-Unyu/Maul4.png.\n",
      "Saved embeddings for Maul-Unyu\n",
      "No faces found in Database/Rahmi/Rahmi.jpg.\n",
      "No faces found in Database/Rahmi/Rahmi2.png.\n",
      "No faces found in Database/Rahmi/Rahmi1.png.\n",
      "No embeddings found for Rahmi. Skipping.\n",
      "Saved embeddings for Devan\n",
      "Saved embeddings for SamSulek\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Step 1: Generate embeddings\n",
    "create_and_store_embeddings(\"Database\")  # Folder with one subfolder per person, each with images of that person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output1.jpg\n"
     ]
    }
   ],
   "source": [
    "recognize_faces(\"Test1.jpeg\", \"output1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output2.jpg\n"
     ]
    }
   ],
   "source": [
    "recognize_faces(\"Test2.jpeg\", \"output2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output3.jpg\n"
     ]
    }
   ],
   "source": [
    "recognize_faces(\"Test3.jpg\", \"output3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to output4.jpg\n"
     ]
    }
   ],
   "source": [
    "recognize_faces(\"Test4.jpg\", \"output4.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Version??? ;(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pls no more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
